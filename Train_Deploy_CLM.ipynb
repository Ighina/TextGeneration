{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_Deploy_CLM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Test the Casual Language Model for conditioned text generation"
      ],
      "metadata": {
        "id": "KDiy6RJ9CMck"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdmBkmQ_zyom",
        "outputId": "654ce0aa-be10-4fc9-ab27-04785d23362c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-rx06f4zk\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-rx06f4zk\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.0.dev0) (4.10.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.0.dev0) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.0.dev0) (3.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.0.dev0) (4.62.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 3.2 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 14.5 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 64.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.0.dev0) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 66.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.0.dev0) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.16.0.dev0) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.16.0.dev0) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.0.dev0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.0.dev0) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.0.dev0) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.0.dev0) (1.15.0)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.16.0.dev0-py3-none-any.whl size=3524819 sha256=62206e785261088317312a9e383ef3378c14771c3b7a070f86c4692c08be45af\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mm5bvsri/wheels/35/2e/a7/d819e3310040329f0f47e57c9e3e7a7338aa5e74c49acfe522\n",
            "Successfully built transformers\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.4 transformers-4.16.0.dev0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-3SYBBO2lZL",
        "outputId": "0b2e9766-1247-407d-e00c-46a8d92b3c4e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-1.18.0-py3-none-any.whl (311 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 28.2 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 61 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 81 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 102 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 112 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 122 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 133 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 143 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 153 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 163 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 194 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 204 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 215 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 225 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 235 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 245 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 256 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 266 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 276 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 286 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 296 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 307 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 311 kB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 36.1 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 80.5 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 52.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.10)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 52.3 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 62.1 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, datasets\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-1.18.0 frozenlist-1.3.0 fsspec-2022.1.0 multidict-6.0.2 xxhash-2.0.2 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pyarrow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bF0WoRgkz1q6",
        "outputId": "df3166a8-9974-4f05-e924-a2b077cd5448"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Collecting pyarrow\n",
            "  Downloading pyarrow-6.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.6 MB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pyarrow) (1.19.5)\n",
            "Installing collected packages: pyarrow\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 3.0.0\n",
            "    Uninstalling pyarrow-3.0.0:\n",
            "      Successfully uninstalled pyarrow-3.0.0\n",
            "Successfully installed pyarrow-6.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.chdir(\"drive/MyDrive/Chatbot-Project/\")\n"
      ],
      "metadata": {
        "id": "tQhqtjA_z_6Z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below code can be uncommented or substitute with relevant files. In my folder, the files were already downloaded so the code is inactive."
      ],
      "metadata": {
        "id": "onG581XN_5hJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXe24QMPz4r5",
        "outputId": "8613b7f1-96cd-402a-91a6-65c7ab05a737"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-24 16:08:05--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2022-01-24 16:08:05 (18.2 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9Vy7SEu0dJC",
        "outputId": "4b1385d3-fa36-4f3f-cb5f-7fade2745312"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘output’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, uncomment this line below if you didn't download the training script from huggingface. This training script will perform all the necessary training and create the model card to be deployed to huggingface. The model will be stored in the output folder inside your drive."
      ],
      "metadata": {
        "id": "-Dh2xlAvAJUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/language-modeling/run_clm.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdib9udB0gC5",
        "outputId": "b4e0d9cf-5fab-4b56-bfae-3eb8032d2fe2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-24 16:10:31--  https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/language-modeling/run_clm.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22736 (22K) [text/plain]\n",
            "Saving to: ‘run_clm.py’\n",
            "\n",
            "\rrun_clm.py            0%[                    ]       0  --.-KB/s               \rrun_clm.py          100%[===================>]  22.20K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2022-01-24 16:10:31 (14.8 MB/s) - ‘run_clm.py’ saved [22736/22736]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_clm.py \\\n",
        "    --output_dir=output \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path=gpt2 \\\n",
        "    --do_train \\\n",
        "    --train_file='input.txt' \\\n",
        "    --per_gpu_train_batch_size=1 \\\n",
        "    --save_steps=-1 \\\n",
        "    --num_train_epochs=2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TLLAidb0mPa",
        "outputId": "f26ab62f-0b69-40aa-dab6-be7ac0e82dff"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "01/24/2022 16:12:42 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "01/24/2022 16:12:42 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=output/runs/Jan24_16-12-42_e375a5e5a3f6,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=2.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=output,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=output,\n",
            "save_on_each_node=False,\n",
            "save_steps=-1,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "01/24/2022 16:12:42 - WARNING - datasets.builder - Using custom data configuration default-4e3f9920f97c584c\n",
            "01/24/2022 16:12:42 - INFO - datasets.builder - Generating dataset text (/root/.cache/huggingface/datasets/text/default-4e3f9920f97c584c/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4)\n",
            "Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-4e3f9920f97c584c/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4...\n",
            "100% 1/1 [00:00<00:00, 3477.86it/s]\n",
            "01/24/2022 16:12:42 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "01/24/2022 16:12:42 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "100% 1/1 [00:00<00:00, 97.74it/s]\n",
            "01/24/2022 16:12:42 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "01/24/2022 16:12:42 - INFO - datasets.builder - Generating split train\n",
            "01/24/2022 16:12:42 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-4e3f9920f97c584c/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 188.85it/s]\n",
            "01/24/2022 16:12:42 - WARNING - datasets.builder - Using custom data configuration default-4e3f9920f97c584c\n",
            "01/24/2022 16:12:42 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "01/24/2022 16:12:42 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-4e3f9920f97c584c/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4\n",
            "01/24/2022 16:12:42 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-4e3f9920f97c584c/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4)\n",
            "01/24/2022 16:12:42 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-4e3f9920f97c584c/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4\n",
            "01/24/2022 16:12:42 - WARNING - datasets.builder - Using custom data configuration default-4e3f9920f97c584c\n",
            "01/24/2022 16:12:42 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "01/24/2022 16:12:42 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-4e3f9920f97c584c/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4\n",
            "01/24/2022 16:12:42 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-4e3f9920f97c584c/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4)\n",
            "01/24/2022 16:12:42 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-4e3f9920f97c584c/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4\n",
            "[INFO|configuration_utils.py:633] 2022-01-24 16:12:42,744 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:669] 2022-01-24 16:12:42,746 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.16.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:372] 2022-01-24 16:12:42,844 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:633] 2022-01-24 16:12:43,047 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:669] 2022-01-24 16:12:43,048 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.16.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1757] 2022-01-24 16:12:43,758 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1757] 2022-01-24 16:12:43,758 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1757] 2022-01-24 16:12:43,758 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1757] 2022-01-24 16:12:43,758 >> loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1757] 2022-01-24 16:12:43,758 >> loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1757] 2022-01-24 16:12:43,758 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:633] 2022-01-24 16:12:43,964 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:669] 2022-01-24 16:12:43,965 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.16.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1427] 2022-01-24 16:12:44,163 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:1694] 2022-01-24 16:12:46,128 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1703] 2022-01-24 16:12:46,128 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Running tokenizer on dataset:   0% 0/38 [00:00<?, ?ba/s]01/24/2022 16:12:46 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-4e3f9920f97c584c/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4/cache-6050ebced5b9944f.arrow\n",
            "Running tokenizer on dataset: 100% 38/38 [00:01<00:00, 30.50ba/s]\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]01/24/2022 16:12:47 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-4e3f9920f97c584c/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4/cache-5b1d036cd7fa9526.arrow\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00, 39.87ba/s]\n",
            "Grouping texts in chunks of 1024:   0% 0/38 [00:00<?, ?ba/s]01/24/2022 16:12:47 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-4e3f9920f97c584c/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4/cache-cd95018e73bad880.arrow\n",
            "Grouping texts in chunks of 1024: 100% 38/38 [00:00<00:00, 66.10ba/s]\n",
            "Grouping texts in chunks of 1024:   0% 0/2 [00:00<?, ?ba/s]01/24/2022 16:12:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-4e3f9920f97c584c/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4/cache-a13aac53e2cdb785.arrow\n",
            "Grouping texts in chunks of 1024: 100% 2/2 [00:00<00:00, 70.44ba/s]\n",
            "[WARNING|training_args.py:975] 2022-01-24 16:12:48,174 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:975] 2022-01-24 16:12:48,174 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1244] 2022-01-24 16:12:48,179 >> ***** Running training *****\n",
            "[INFO|trainer.py:1245] 2022-01-24 16:12:48,179 >>   Num examples = 300\n",
            "[INFO|trainer.py:1246] 2022-01-24 16:12:48,179 >>   Num Epochs = 2\n",
            "[INFO|trainer.py:1247] 2022-01-24 16:12:48,179 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1248] 2022-01-24 16:12:48,179 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:1249] 2022-01-24 16:12:48,179 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1250] 2022-01-24 16:12:48,179 >>   Total optimization steps = 600\n",
            "[WARNING|training_args.py:975] 2022-01-24 16:12:48,194 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "{'loss': 3.4813, 'learning_rate': 8.333333333333334e-06, 'epoch': 1.67}\n",
            "100% 600/600 [3:29:36<00:00, 20.97s/it][INFO|trainer.py:1473] 2022-01-24 19:42:24,756 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 12576.5775, 'train_samples_per_second': 0.048, 'train_steps_per_second': 0.048, 'train_loss': 3.454905497233073, 'epoch': 2.0}\n",
            "100% 600/600 [3:29:36<00:00, 20.96s/it]\n",
            "[INFO|trainer.py:2086] 2022-01-24 19:42:24,762 >> Saving model checkpoint to output\n",
            "[INFO|configuration_utils.py:431] 2022-01-24 19:42:24,768 >> Configuration saved in output/config.json\n",
            "[INFO|modeling_utils.py:1074] 2022-01-24 19:42:27,923 >> Model weights saved in output/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2060] 2022-01-24 19:42:27,944 >> tokenizer config file saved in output/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2066] 2022-01-24 19:42:27,949 >> Special tokens file saved in output/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        2.0\n",
            "  train_loss               =     3.4549\n",
            "  train_runtime            = 3:29:36.57\n",
            "  train_samples            =        300\n",
            "  train_samples_per_second =      0.048\n",
            "  train_steps_per_second   =      0.048\n",
            "[WARNING|training_args.py:975] 2022-01-24 19:42:28,130 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:975] 2022-01-24 19:42:28,131 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|modelcard.py:460] 2022-01-24 19:42:28,253 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "after training you can try the model by loading it directly from the output folder in which it was stored."
      ],
      "metadata": {
        "id": "PgFb_aNBAiow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        " \n",
        "tokenizer = GPT2Tokenizer.from_pretrained('output')\n",
        "model = GPT2LMHeadModel.from_pretrained('output')"
      ],
      "metadata": {
        "id": "ztqGXoFqnm3m"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are different types of generation available. here below I show some examples. The last one is probably the best one in terms of originality of the output.\n",
        "In general, you just need to pass an initial input in the tokenizer.encode function and the results of that in the model.generate function with different parameter configurations."
      ],
      "metadata": {
        "id": "_Uy8zCyCAqFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "greedy_outputs = model.generate(tokenizer.encode('[BOS] The King must leave the throne now . [EOS]',\n",
        "                      return_tensors='pt'), max_length = 100)"
      ],
      "metadata": {
        "id": "TdDadh_moAZb",
        "outputId": "bd39ce94-0f1b-4754-9f57-f52631a955fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, greedy_output in enumerate(greedy_outputs):\n",
        "  print(\"\\n\"+\"===\"*10)\n",
        "  print(\"{}: {}\".format(i+1, tokenizer.decode(greedy_output, skip_special_tokens=False)))"
      ],
      "metadata": {
        "id": "7wPIbdHqoRWQ",
        "outputId": "b550d18f-7a39-415f-b373-0d788cce132f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "==============================\n",
            "1: [BOS] The King must leave the throne now. [EOS]\n",
            "\n",
            "KING RICHARD II:\n",
            "I'll not be so bold as to say so.\n",
            "\n",
            "BOS:\n",
            "I'll not be so bold as to say so.\n",
            "\n",
            "KING RICHARD II:\n",
            "I'll not be so bold as to say so.\n",
            "\n",
            "BOS:\n",
            "I'll not be so bold as to say so.\n",
            "\n",
            "KING RICHARD II:\n",
            "I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_outputs = model.generate(tokenizer.encode('[BOS] How are you ? [EOS]',\n",
        "                      return_tensors='pt'), max_length = 100, do_sample = True)"
      ],
      "metadata": {
        "id": "b6rQSB6Sot4O",
        "outputId": "71e6e524-ba9f-4173-fb83-bc13f94370d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"\\n\"+\"===\"*10)\n",
        "  print(\"{}: {}\".format(i+1, tokenizer.decode(greedy_output, skip_special_tokens=False)))"
      ],
      "metadata": {
        "id": "7djlRkKDo4hE",
        "outputId": "6861ab0a-6209-4999-ec62-61e9522edea8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "==============================\n",
            "1: [BOS] How are you? [EOS] How I am to come?\n",
            "\n",
            "PROSPERO:\n",
            "You live, I go to the garden\n",
            "Of the good-tempered mother of mine.\n",
            "\n",
            "KING RICHARD II:\n",
            "What, by nature, is my wife?\n",
            "\n",
            "BOS:\n",
            "In that, I hear, that I live.\n",
            "\n",
            "PROSPERO:\n",
            "O dear, how were you and your wife born!\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set top_k to 50\n",
        "sample_output2 = model.generate(\n",
        "    ids, \n",
        "    do_sample=True, \n",
        "    max_length=300, \n",
        "    top_k=50\n",
        ")\n",
        " \n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output2[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "OKiYkw-BBWoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_output3 = model.generate(\n",
        "    ids, \n",
        "    do_sample=True, \n",
        "    max_length=300, \n",
        "    top_p=0.92,\n",
        ")\n",
        " \n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output3[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "tNWylwfgBeKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_outputs = model.generate(\n",
        "    ids,\n",
        "    do_sample=True, \n",
        "    max_length=300, \n",
        "    top_k=40, \n",
        "    top_p=0.95, \n",
        ")\n",
        " \n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, final_output in enumerate(final_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(final_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "id": "ODDVUyYABirM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deploy the Model to Huggingface hub"
      ],
      "metadata": {
        "id": "w4JqTqx-B0iX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To deploy the model to huggingface hub, follow the instructions from here https://huggingface.co/docs/hub/adding-a-model\n",
        "\n",
        "Once the model is hosted on the platform you can retrieve it directly from there."
      ],
      "metadata": {
        "id": "prxJI1UtFIIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        " \n",
        "tokenizer = GPT2Tokenizer.from_pretrained('Iacopo/Shakespear-GPT2') # use the name of the model as stored on huggingface hub\n",
        "model = GPT2LMHeadModel.from_pretrained('Iacopo/Shakespear-GPT2')"
      ],
      "metadata": {
        "id": "eI7g1HfaCKhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can go back up again and try the model by running the cells starting from the greedy_outputs = ... cell"
      ],
      "metadata": {
        "id": "YQZwtnkRFiwP"
      }
    }
  ]
}